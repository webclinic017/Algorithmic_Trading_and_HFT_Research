{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfUzYIsY1Ooy",
        "outputId": "d7c7495f-b02c-4480-efae-71dd77a25489"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"ip\": \"202.189.111.106\",\n",
            "  \"city\": \"Hong Kong\",\n",
            "  \"region\": \"Hong Kong\",\n",
            "  \"country\": \"HK\",\n",
            "  \"loc\": \"22.2783,114.1747\",\n",
            "  \"org\": \"AS4528 The University of Hong Kong\",\n",
            "  \"postal\": \"999077\",\n",
            "  \"timezone\": \"Asia/Hong_Kong\",\n",
            "  \"readme\": \"https://ipinfo.io/missingauth\"\n",
            "}"
          ]
        }
      ],
      "source": [
        "!curl ipinfo.io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QpAfSIyE8Boh"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import websocket\n",
        "import threading\n",
        "from datetime import datetime as dt, timedelta\n",
        "import os\n",
        "import traceback\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DZ_Kw7teZWHY"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GrossmanMillerModel:\n",
        "    def __init__(self, num_mm, gamma, sigma_squared):\n",
        "        \"\"\"\n",
        "        Initializes the Grossman-Miller model.\n",
        "\n",
        "        Args:\n",
        "            num_mm (int): Number of market makers.\n",
        "            gamma (float): Risk aversion parameter.\n",
        "            sigma_squared (float): Variance of the asset price shock.\n",
        "        \"\"\"\n",
        "        self.num_mm = num_mm\n",
        "        self.gamma = gamma\n",
        "        self.sigma_squared = sigma_squared\n",
        "        self.mu = 0  # Expected asset value (can be dynamic in a more complex model)\n",
        "\n",
        "    def calculate_price_t1(self, i):\n",
        "        \"\"\"\n",
        "        Calculates the equilibrium price at t=1.\n",
        "\n",
        "        Args:\n",
        "            i (float): Liquidity trader's desired trade (positive for sell, negative for buy).\n",
        "\n",
        "        Returns:\n",
        "            float: Equilibrium price at t=1.\n",
        "        \"\"\"\n",
        "        return self.mu - self.gamma * self.sigma_squared * (i / (self.num_mm + 1))\n",
        "\n",
        "    def calculate_quantity_t1(self, i):\n",
        "        \"\"\"\n",
        "        Calculates the quantity of asset held by each MM and LT1 at t=1.\n",
        "\n",
        "        Args:\n",
        "            i (float): Liquidity trader's desired trade.\n",
        "\n",
        "        Returns:\n",
        "            float: Quantity held by each agent at t=1.\n",
        "        \"\"\"\n",
        "        return i / (self.num_mm + 1)\n",
        "\n",
        "    def calculate_price_impact(self, i):\n",
        "        \"\"\"\n",
        "        Calculates the price impact (lambda) and the actual trade quantity of LT1.\n",
        "\n",
        "        Args:\n",
        "            i (float): Liquidity trader's desired trade.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing lambda and the trade quantity of LT1.\n",
        "        \"\"\"\n",
        "\n",
        "        trade_quantity_lt1 = i * self.num_mm / (self.num_mm + 1)\n",
        "        price_impact = -(1 / self.num_mm) * self.gamma * self.sigma_squared\n",
        "        return price_impact, trade_quantity_lt1\n",
        "\n",
        "\n",
        "    def run_simulation(self, trades):\n",
        "        \"\"\"\n",
        "        Runs a simulation of the Grossman-Miller model over a series of trades.\n",
        "\n",
        "        Args:\n",
        "            trades (list): A list of liquidity trader trades (i values).\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: A DataFrame containing the simulation results.\n",
        "        \"\"\"\n",
        "        prices_t1 = []\n",
        "        quantities_t1 = []\n",
        "        price_impacts = []\n",
        "        trade_quantities = []\n",
        "\n",
        "        for i in trades:\n",
        "            price_t1 = self.calculate_price_t1(i)\n",
        "            quantity_t1 = self.calculate_quantity_t1(i)\n",
        "            price_impact, trade_quantity_lt1 = self.calculate_price_impact(i)\n",
        "\n",
        "            prices_t1.append(price_t1)\n",
        "            quantities_t1.append(quantity_t1)\n",
        "            price_impacts.append(price_impact)\n",
        "            trade_quantities.append(trade_quantity_lt1)\n",
        "\n",
        "        results = pd.DataFrame({\n",
        "            \"trade\": trades,\n",
        "            \"price_t1\": prices_t1,\n",
        "            \"quantity_t1\": quantities_t1,\n",
        "            \"price_impact\": price_impacts,\n",
        "            \"trade_quantity_lt1\": trade_quantities\n",
        "        })\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FJCfI8aKh92E"
      },
      "outputs": [],
      "source": [
        "class MarketDataCollector:\n",
        "    def __init__(self, data_path='crypto_data'):\n",
        "        self.combined_data = []\n",
        "        self.data_path = data_path\n",
        "        self.running = True\n",
        "\n",
        "        if not os.path.exists(data_path):\n",
        "            os.makedirs(data_path)\n",
        "            print(f\"Created directory: {data_path}\")\n",
        "        else:\n",
        "            print(f\"Using existing directory: {data_path}\")\n",
        "\n",
        "    def save_data(self, df, filename):\n",
        "        try:\n",
        "            full_path = os.path.join(self.data_path, filename)\n",
        "            print(f\"Saving data to {full_path}...\")\n",
        "            df.to_csv(full_path, index=False)  #\n",
        "            print(f\"Successfully saved {len(df)} records\")\n",
        "        except Exception as e:\n",
        "            print(f\"Save failed: {str(e)[:200]}\")\n",
        "\n",
        "\n",
        "    def _ws_handler(self, message, data_type):\n",
        "        try:\n",
        "            data = json.loads(message)\n",
        "            timestamp = dt.fromtimestamp(data['E']/1000).strftime(\"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
        "\n",
        "            # Add safety checks for order book data\n",
        "            bid_price = None\n",
        "            ask_price = None\n",
        "            if data_type == 'depth':\n",
        "                if len(data.get('b', [])) > 0:\n",
        "                    bid_price = float(data['b'][0][0])\n",
        "                if len(data.get('a', [])) > 0:\n",
        "                    ask_price = float(data['a'][0][0])\n",
        "\n",
        "            record = {\n",
        "                'timestamp': timestamp,\n",
        "                'bid_price': bid_price,\n",
        "                'ask_price': ask_price,\n",
        "                'trade_price': float(data['p']) if data_type == 'trade' else None,\n",
        "                'volume': float(data['q'])*float(data['p']) if data_type == 'trade' else None\n",
        "            }\n",
        "            self.combined_data.append(record)\n",
        "        except Exception as e:\n",
        "            print(f\"Processing error: {str(e)[:200]}\")\n",
        "\n",
        "\n",
        "    def on_order_book_message(self, ws, message):\n",
        "        self._ws_handler(message, 'depth')\n",
        "\n",
        "    def on_trade_message(self, ws, message):\n",
        "        self._ws_handler(message, 'trade')\n",
        "\n",
        "\n",
        "    def _run_websocket(self, url, handler, stream_type):\n",
        "        end_time = time.time() + (3600 * 24)  # Fail-safe timeout\n",
        "        while time.time() < end_time and self.running:\n",
        "            try:\n",
        "                ws = websocket.WebSocketApp(\n",
        "                    url,\n",
        "                    on_message=handler,\n",
        "                    on_error=lambda ws, e: print(f\"{stream_type} error: {str(e)[:200]}\"),\n",
        "                    on_close=lambda ws: print(f\"{stream_type} closed\"),\n",
        "                    on_open=lambda ws: print(f\"{stream_type} connected\")\n",
        "                )\n",
        "                ws.run_forever(ping_interval=30, ping_timeout=10)\n",
        "            except Exception as e:\n",
        "                print(f\"{stream_type} failure: {str(e)[:200]}\")\n",
        "            time.sleep(5)\n",
        "\n",
        "    def collect_long_duration_data(self, symbol, duration_hours=0.1, checkpoint_minutes=1):\n",
        "\n",
        "        total_seconds = duration_hours * 3600\n",
        "        checkpoint_seconds = checkpoint_minutes * 60\n",
        "        start_time = time.time()\n",
        "        end_time = start_time + total_seconds\n",
        "        last_checkpoint = start_time\n",
        "        checkpoint_count = 0\n",
        "\n",
        "        print(f\"\\n{'='*40}\\nStarting {duration_hours}-hour collection for {symbol}\")\n",
        "        print(f\"Checkpoints every {checkpoint_minutes} mins | Target end: {dt.fromtimestamp(end_time)}\\n{'='*40}\")\n",
        "\n",
        "\n",
        "        threads = [\n",
        "            threading.Thread(target=self._run_websocket,\n",
        "                args=(f\"wss://stream.binance.com:9443/ws/{symbol}@depth@100ms\",\n",
        "                    self.on_order_book_message, \"OrderBook\")),\n",
        "            threading.Thread(target=self._run_websocket,\n",
        "                args=(f\"wss://stream.binance.com:9443/ws/{symbol}@trade\",\n",
        "                    self.on_trade_message, \"Trades\"))\n",
        "        ]\n",
        "\n",
        "        for t in threads:\n",
        "            t.daemon = True\n",
        "            t.start()\n",
        "\n",
        "        alive_threads = sum(1 for t in threads if t.is_alive())\n",
        "        print(f\"Active connections: {alive_threads}/2 | Buffer size: {len(self.combined_data)}\")\n",
        "\n",
        "        try:\n",
        "            while time.time() < end_time and self.running:\n",
        "                # !IMPROVEMENT: Adaptive sleep management\n",
        "                remaining = end_time - time.time()\n",
        "                sleep_time = max(0, min(1, remaining))\n",
        "                time.sleep(sleep_time)\n",
        "\n",
        "                # Checkpoint handling\n",
        "                if time.time() - last_checkpoint >= checkpoint_seconds:\n",
        "                    checkpoint_count += 1\n",
        "                    last_checkpoint = time.time()\n",
        "                    self._process_checkpoint(symbol, checkpoint_count)\n",
        "\n",
        "                if int(time.time() - start_time) % 10 == 0:\n",
        "                    elapsed = time.time() - start_time\n",
        "                    progress = min(100, (elapsed / total_seconds) * 100)\n",
        "                    print(f\"Progress: {progress:.1f}% | Records: {len(self.combined_data)}\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nUser requested shutdown...\")\n",
        "        finally:\n",
        "            self.running = False\n",
        "            self._final_save(symbol, duration_hours)\n",
        "            print(\"\\nCollection completed\" if time.time() >= end_time else \"\\nCollection aborted\")\n",
        "\n",
        "    def _process_checkpoint(self, symbol, count):\n",
        "        try:\n",
        "            print(f\"\\n{'='*20} Checkpoint {count} {'='*20}\")\n",
        "            original_count = len(self.combined_data)\n",
        "\n",
        "            # Process COPY of data\n",
        "            temp_data = self.combined_data.copy()\n",
        "            df = self.process_data()\n",
        "\n",
        "            if not df.empty:\n",
        "                # Only clear original data AFTER successful processing\n",
        "                self.combined_data = self.combined_data[original_count:]  # Keep unprocessed data\n",
        "                timestamp = dt.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                filename = f\"{symbol}_checkpoint_{count}_{timestamp}.csv\"\n",
        "                self.save_data(df, filename)\n",
        "\n",
        "            # Memory management (preserve last 10k)\n",
        "            if len(self.combined_data) > 10000:\n",
        "                self.combined_data = self.combined_data[-10000:]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Checkpoint failed: {str(e)[:200]}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "\n",
        "    def process_data(self):\n",
        "        try:\n",
        "            if not self.combined_data:\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            batch = pd.DataFrame(self.combined_data.copy())\n",
        "            batch['timestamp'] = pd.to_datetime(batch['timestamp'])\n",
        "            batch = batch.drop_duplicates(subset=['timestamp'], keep='last')\n",
        "\n",
        "            # ==== New Validation Checks ====\n",
        "            required_columns = {'bid_price', 'ask_price', 'trade_price', 'volume'}\n",
        "            if not required_columns.issubset(batch.columns):\n",
        "                missing = required_columns - set(batch.columns)\n",
        "                print(f\"Missing columns: {missing}\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            if not batch.empty:\n",
        "                time_span = batch['timestamp'].max() - batch['timestamp'].min()\n",
        "                if time_span < pd.Timedelta('1s'):\n",
        "                    print(f\"Critical time range error: {time_span}\")\n",
        "                    return pd.DataFrame()\n",
        "            # ==============================\n",
        "\n",
        "            if not batch.empty:\n",
        "                resampled = (\n",
        "                    batch.set_index('timestamp')\n",
        "                    .resample('1000ms', origin='start')\n",
        "                    .agg({\n",
        "                        'bid_price': 'ffill',\n",
        "                        'ask_price': 'ffill',\n",
        "                        'trade_price': 'bfill',\n",
        "                        'volume': 'sum'\n",
        "                    })\n",
        "                    .reset_index()\n",
        "                )\n",
        "                resampled['mid_price'] = (resampled['bid_price'] + resampled['ask_price']) / 2\n",
        "                resampled = resampled.ffill().dropna(subset=['bid_price', 'ask_price'], how='all')\n",
        "\n",
        "                print(f\"Processed {len(batch)} records -> {len(resampled)} data points\")\n",
        "                return resampled\n",
        "            return pd.DataFrame()\n",
        "        except Exception as e:\n",
        "            print(f\"Processing error: {str(e)[:200]}\")\n",
        "            traceback.print_exc()\n",
        "            return pd.DataFrame()\n",
        "\n",
        "\n",
        "\n",
        "    def _final_save(self, symbol, duration):\n",
        "\n",
        "        try:\n",
        "            print(\"\\nFinalizing collection...\")\n",
        "            df = self.process_data()\n",
        "            if not df.empty:\n",
        "                timestamp = dt.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                filename = f\"{symbol}_FINAL_{duration}h_{timestamp}.csv\"\n",
        "                self.save_data(df, filename)\n",
        "        except Exception as e:\n",
        "            print(f\"Final save failed: {str(e)[:200]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcKrZT4mh0tT",
        "outputId": "1fec73b3-1885-44dd-b88b-783db576e704"
      },
      "outputs": [],
      "source": [
        "def backtest_grossman_miller(symbol, duration_hours=0.1, gamma=1, sigma_squared=0.01, \n",
        "                            num_mm=10, csv_path=None, resample_freq=None):\n",
        "    \"\"\"\n",
        "    Fetches market data (or loads from CSV), simulates the Grossman-Miller model, and analyzes the results.\n",
        "    \n",
        "    Args:\n",
        "        symbol (str): The trading symbol (e.g., \"btcusdt\").\n",
        "        duration_hours (float): Duration for data collection (ignored if csv_path is provided).\n",
        "        gamma (float): Risk aversion parameter for the model.\n",
        "        sigma_squared (float): Variance of price shock for the model.\n",
        "        num_mm (int): Number of market makers for the model.\n",
        "        csv_path (str, optional): Path to a CSV file with market data.\n",
        "        resample_freq (str, optional): Frequency to resample data to (e.g., \"1s\", \"100ms\")\n",
        "    \"\"\"\n",
        "    \n",
        "    if csv_path and os.path.exists(csv_path):\n",
        "        print(f\"Loading market data from {csv_path}\")\n",
        "        try:\n",
        "            # Load data from CSV file\n",
        "            processed_data = pd.read_csv(csv_path)\n",
        "            \n",
        "            # Convert timestamp to datetime if it's not already\n",
        "            if 'timestamp' in processed_data.columns and not pd.api.types.is_datetime64_any_dtype(processed_data['timestamp']):\n",
        "                processed_data['timestamp'] = pd.to_datetime(processed_data['timestamp'])\n",
        "            \n",
        "            # Required columns check\n",
        "            required_columns = ['bid_price', 'ask_price', 'trade_price', 'volume']\n",
        "            missing_columns = [col for col in required_columns if col not in processed_data.columns]\n",
        "            \n",
        "            if missing_columns:\n",
        "                print(f\"Warning: Missing required columns in CSV: {missing_columns}\")\n",
        "                \n",
        "                # Calculate mid_price if needed\n",
        "                if 'mid_price' not in processed_data.columns and 'bid_price' in processed_data.columns and 'ask_price' in processed_data.columns:\n",
        "                    processed_data['mid_price'] = (processed_data['bid_price'] + processed_data['ask_price']) / 2\n",
        "                    print(\"Calculated mid_price from bid_price and ask_price\")\n",
        "                    \n",
        "            print(f\"Loaded {len(processed_data)} records from CSV\")\n",
        "            \n",
        "            # Resample the data if requested\n",
        "            if resample_freq and 'timestamp' in processed_data.columns:\n",
        "                print(f\"Resampling data to {resample_freq} frequency\")\n",
        "                processed_data = processed_data.set_index('timestamp')\n",
        "                \n",
        "                # Calculate average time between records for diagnostics\n",
        "                avg_time_delta = (processed_data.index.max() - processed_data.index.min()) / len(processed_data)\n",
        "                print(f\"Average time between records: {avg_time_delta}\")\n",
        "                \n",
        "                # Resample with appropriate aggregation methods\n",
        "                processed_data = processed_data.resample(resample_freq).agg({\n",
        "                    'bid_price': 'last',\n",
        "                    'ask_price': 'last',\n",
        "                    'trade_price': 'last',\n",
        "                    'volume': 'sum',\n",
        "                    'mid_price': 'last' if 'mid_price' in processed_data.columns else None\n",
        "                }).dropna()\n",
        "                \n",
        "                # Recalculate mid_price if needed\n",
        "                if 'mid_price' not in processed_data.columns:\n",
        "                    processed_data['mid_price'] = (processed_data['bid_price'] + processed_data['ask_price']) / 2\n",
        "                    \n",
        "                processed_data = processed_data.reset_index()\n",
        "                print(f\"After resampling: {len(processed_data)} records\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data from CSV: {str(e)}\")\n",
        "            print(\"Falling back to real-time data collection...\")\n",
        "            csv_path = None\n",
        "    \n",
        "    # Real-time data collection fallback\n",
        "    if not csv_path or not os.path.exists(csv_path):\n",
        "        print(f\"Collecting real-time market data for {symbol}...\")\n",
        "        collector = MarketDataCollector()\n",
        "        collector.collect_long_duration_data(symbol, duration_hours=duration_hours, checkpoint_minutes=1)\n",
        "        processed_data = collector.process_data()\n",
        "    \n",
        "    if processed_data.empty:\n",
        "        print(\"No data to backtest.\")\n",
        "        return\n",
        "    \n",
        "    # Determine data frequency for parameter scaling\n",
        "    if 'timestamp' in processed_data.columns:\n",
        "        processed_data['time_diff'] = processed_data['timestamp'].diff()\n",
        "        median_time_diff = processed_data['time_diff'].median()\n",
        "        print(f\"Median time between records: {median_time_diff}\")\n",
        "        \n",
        "        # Scale sigma_squared based on data frequency\n",
        "        if median_time_diff < pd.Timedelta('500ms'):\n",
        "            # High-frequency data needs reduced sigma_squared\n",
        "            original_sigma = sigma_squared\n",
        "            sigma_squared = sigma_squared * 0.1  # Scale down for high frequency\n",
        "            print(f\"High-frequency data detected. Scaling sigma_squared from {original_sigma} to {sigma_squared}\")\n",
        "    \n",
        "    # Adaptive threshold calculation based on data characteristics\n",
        "    volume_std = processed_data['volume'].std()\n",
        "    volume_mean = processed_data['volume'].mean()\n",
        "    \n",
        "    # Use higher threshold for noisier data\n",
        "    if volume_std > volume_mean * 10:\n",
        "        threshold = processed_data['volume'].quantile(0.75)  # 75th percentile for high variation\n",
        "        print(f\"High volume variation detected. Using 75th percentile threshold: {threshold}\")\n",
        "    else:\n",
        "        threshold = volume_mean\n",
        "        print(f\"Using mean volume threshold: {threshold}\")\n",
        "    \n",
        "    # Generate trade sizes with frequency-appropriate scaling\n",
        "    trades = []\n",
        "    for index, row in processed_data.iterrows():\n",
        "        trade_size = 0\n",
        "        if pd.notna(row['volume']) and row['volume'] > threshold:\n",
        "            # Adjust divisor based on price level\n",
        "            price_level = row['mid_price'] if pd.notna(row['mid_price']) else row['trade_price']\n",
        "            divisor = 10 if price_level < 100 else 100\n",
        "            \n",
        "            # Scale for high-frequency data\n",
        "            if 'time_diff' in processed_data.columns and median_time_diff < pd.Timedelta('500ms'):\n",
        "                divisor *= 10  # Further reduce trade sizes for millisecond data\n",
        "                \n",
        "            trade_size = (row['volume'] - threshold) / divisor\n",
        "        trades.append(trade_size)\n",
        "    \n",
        "    # Create model and run simulation with adjusted parameters\n",
        "    model = GrossmanMillerModel(num_mm, gamma, sigma_squared)\n",
        "    results = model.run_simulation(trades)\n",
        "    \n",
        "    # Ensure dataframes align\n",
        "    if 'timestamp' in processed_data.columns:\n",
        "        processed_data = processed_data.reset_index(drop=True)\n",
        "    \n",
        "    if len(results) != len(processed_data):\n",
        "        min_len = min(len(results), len(processed_data))\n",
        "        results = results.iloc[:min_len]\n",
        "        processed_data = processed_data.iloc[:min_len]\n",
        "    \n",
        "    # Combine results and calculate PnL\n",
        "    combined_results = pd.concat([processed_data, results], axis=1)\n",
        "    combined_results['price_change'] = combined_results['mid_price'].diff()\n",
        "    combined_results['mm_pnl'] = -combined_results['price_impact'] * combined_results['trade_quantity_lt1'] * combined_results['price_change']\n",
        "    \n",
        "    # Diagnostic statistics\n",
        "    print(\"\\nMarket Making Performance Metrics:\")\n",
        "    print(f\"Total trades: {len(combined_results[combined_results['trade'] > 0])}\")\n",
        "    print(f\"Average trade size: {combined_results['trade'].mean():.6f}\")\n",
        "    print(f\"Average price change: {combined_results['price_change'].mean():.6f}\")\n",
        "    print(f\"Price change volatility: {combined_results['price_change'].std():.6f}\")\n",
        "    print(f\"Total market maker PnL: {combined_results['mm_pnl'].sum():.2f}\")\n",
        "    \n",
        "    # Return full results for further analysis\n",
        "    return combined_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading market data from /home/misango/code/Algorithmic_Trading_and_HFT_Research/Market_Making/Avellaneda-Stoikov/Data_Folder_Test/HFT_2_hr_combined_crypto_data.csv\n",
            "Loaded 120677 records from CSV\n",
            "Resampling data to 100ms frequency\n",
            "Average time between records: 0 days 00:00:00.059659255\n",
            "After resampling: 71796 records\n",
            "Median time between records: 0 days 00:00:00.100000\n",
            "High-frequency data detected. Scaling sigma_squared from 0.001 to 0.0001\n",
            "High volume variation detected. Using 75th percentile threshold: 90.017426625\n",
            "\n",
            "Market Making Performance Metrics:\n",
            "Total trades: 17949\n",
            "Average trade size: 1.972182\n",
            "Average price change: -0.001403\n",
            "Price change volatility: 2428.131614\n",
            "Total market maker PnL: 20.05\n",
            "Loading market data from /home/misango/code/Algorithmic_Trading_and_HFT_Research/Market_Making/Avellaneda-Stoikov/Data_Folder_Test/HFT_2_hr_combined_crypto_data.csv\n",
            "Loaded 120677 records from CSV\n",
            "Resampling data to 1s frequency\n",
            "Average time between records: 0 days 00:00:00.059659255\n",
            "After resampling: 7199 records\n",
            "Median time between records: 0 days 00:00:01\n",
            "Using mean volume threshold: 19937.134863574134\n",
            "\n",
            "Market Making Performance Metrics:\n",
            "Total trades: 974\n",
            "Average trade size: 142.437828\n",
            "Average price change: -0.013995\n",
            "Price change volatility: 2453.814461\n",
            "Total market maker PnL: 53503.63\n",
            "100ms data PnL: 20.05\n",
            "1s data PnL: 53503.63\n"
          ]
        }
      ],
      "source": [
        "results_100ms = backtest_grossman_miller(\n",
        "    symbol=\"btcusdt\",\n",
        "    gamma=1,\n",
        "    sigma_squared=0.001,\n",
        "    num_mm=10,\n",
        "    csv_path=\"/home/misango/code/Algorithmic_Trading_and_HFT_Research/Market_Making/Avellaneda-Stoikov/Data_Folder_Test/HFT_2_hr_combined_crypto_data.csv\",\n",
        "    resample_freq=\"100ms\"  # Resample to 100ms intervals\n",
        ")\n",
        "\n",
        "results_1s = backtest_grossman_miller(\n",
        "    symbol=\"btcusdt\",\n",
        "    gamma=1,\n",
        "    sigma_squared=0.01,  # Higher for second-level data\n",
        "    num_mm=10,\n",
        "    csv_path=\"/home/misango/code/Algorithmic_Trading_and_HFT_Research/Market_Making/Avellaneda-Stoikov/Data_Folder_Test/HFT_2_hr_combined_crypto_data.csv\",\n",
        "    resample_freq=\"1s\"  # Resample to 1-second intervals\n",
        ")\n",
        "print(f\"100ms data PnL: {results_100ms['mm_pnl'].sum():.2f}\")\n",
        "print(f\"1s data PnL: {results_1s['mm_pnl'].sum():.2f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "coding_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
